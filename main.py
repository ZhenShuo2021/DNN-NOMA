# -*- coding: utf-8 -*-
"""
Created on Sun Jan 23 14:05:02 2022

@author: Leo
"""
from tensorflow.keras.models import Model
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.optimizers import Adam

import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import time
import ray


# %%
# Params
m = 70          # num of subcarriers
N = 100         # num of total users
Nd = 7          # num of "multiple measurement"
alpha = 9*N    # num of neurons each dense layer
snr = 10        # training SNR
p = 1024       # num of data generated at one time (depend on memory)

k = 8           # num of active user
dv = 7         # num of spreading code for each user


# %%
# Functions
def SpreadingCodeGen(N, m, Nd, dv):
    index = np.zeros((dv, N), dtype=int)
    index_list = []
    for i in range(N):
        index[:, i] = np.random.choice(m, dv, replace=False)
        index[:, i] = np.sort(index[:, i])

        index_list.append(index[:, i].tolist())
        for j in range(i):
            while index_list[j] == index_list[i]:
                index_list[j] = np.sort(np.random.choice(
                    m, dv, replace=False)).tolist()
    return index_list


def Hidden_Layer(input_tensor, alpha, stage):
    """
    Parameters
    ----------
    input_tensor : Output of last layer
    alpha : Number of neuron
    stage : Index of hidden layer

    Returns output tensor
    -------
    """
    name_base = 'HL' + stage
    x = layers.Dense(alpha, name=name_base + '_1',
                     # kernel_initializer='ones',
                     # kernel_regularizer=tf.keras.regularizers.l1(0.0001),
                     activity_regularizer=tf.keras.regularizers.l2(0.00005)
                     )(input_tensor)
    # x = layers.Dense(alpha, name=name_base + '_1')(input_tensor)
    x = layers.BatchNormalization(name=name_base + '_2')(x)
    x = layers.Activation('relu', name=name_base + '_3')(x)
    x = layers.Dropout(0.1, name=name_base + '_4')(x)
    x = layers.add([x, input_tensor], name=name_base + '_Add')
    return x


def AUD(alpha, N, m):
    model_input = layers.Input(shape=[2*m, ], name='InputLayer')
    x = layers.Dense(alpha, name='InputFC')(model_input)
    x = layers.BatchNormalization(name='InputBN')(x)
    x = Hidden_Layer(x, alpha, stage='_A')
    x = Hidden_Layer(x, alpha, stage='_B')
    x = Hidden_Layer(x, alpha, stage='_C')
    x = Hidden_Layer(x, alpha, stage='_D')
    x = Hidden_Layer(x, alpha, stage='_E')
    x = Hidden_Layer(x, alpha, stage='_F')
    # x = Hidden_Layer(x, alpha, stage='_G')
    # x = Hidden_Layer(x, alpha, stage='_H')
    # x = Hidden_Layer(x, alpha, stage='_I')
    # x = Hidden_Layer(x, alpha, stage='_J')
    # x = Hidden_Layer(x, alpha, stage='_K')
    # x = Hidden_Layer(x, alpha, stage='_L')

    x = layers.Dense(N, name='OutputFC')(x)
    x = layers.Softmax(name='OutputActivatio')(x)

    model = Model(model_input, x, name='D_AUD')
    return model


def NonDiagCodebook(N, m, Nd, dv, SC):
    # Codebook = np.zeros((100,70,70))   # Followed by paper (diagonal)
    Codebook = np.zeros((N, m, 1))   # Modified, not doing diagonalize.

    for i in range(N):
        # Followed by paper (diagonal)
        # temp = np.zeros((70,1))
        # temp[SC[i],0] = 1
        # Codebook[i,:,:] = np.diag(temp[:,0])   # Followed by paper (diagonal)
        Codebook[i, :, :][SC[i], 0] = 1

    Codebook = np.tile(Codebook, [Nd, 1])
    return Codebook


def TrainingGen(N, m, Nd, dv, p, k, snr):
    '''
    p: How much data generated in a row (memory depended)
    '''
    # row: user, col: Nd symbols with m channel gains
    active_index_matrix = np.zeros((k, p), dtype='int32')   # List
    # np array generated by active_index_matrix
    active_delta_matrix = np.zeros((N, p), dtype='int32')
    y_hat_p = np.zeros((2*m, p))

    for i in range(p):
        active_index = np.random.choice(N, k, replace=False)
        active_index_matrix[:, i] = active_index
        active_delta_matrix[:, i][active_index] = 1

    y_tilde = np.zeros((m, p), dtype='complex64')
    for i in range(p):
        # print(i)
        x = np.zeros((N, m*Nd), dtype='complex64')
        for j in (active_index_matrix[:, i]):
            bits = np.ones((1, Nd))
            channel = np.random.randn(1, m) + 1j*np.random.randn(1, m)
            channel = channel * np.sqrt(1/2)
            # channel = np.ones((1, m))
            x[j, :] = np.kron(bits, channel)
            x[j, :] = x[j, :]*np.squeeze(Codebook[j, :, :])
        x = np.sum(x, axis=0)   # Sum all user data to each subcarrier
        x = np.split(x, Nd)   # Split and sum all measurments to one subcarrier
        y_tilde[:, i] = np.sum(x, axis=0)

    rxPwr = np.mean(np.square(np.abs(y_tilde)))
    # noisePwr = 10**(rxPwr) - 10**(snr/10)
    noiseLinear = rxPwr / 10**(snr/10)
    noise = np.sqrt(noiseLinear/2) * (np.random.randn(*y_tilde.shape,) +
                                      1j*np.random.randn(*y_tilde.shape,))

    # noise = np.sqrt(noiseLinear/2) * \
    #     np.zeros(y_tilde.shape,)    # check (no noise)
    # noisePwr = np.mean(np.square(np.abs(noise)))   # check (noise power)

    y_tilde = y_tilde + noise

    y_hat_p[:m, :] = np.real(y_tilde)
    y_hat_p[m:, :] = np.imag(y_tilde)

    y_hat_p = y_hat_p.T
    active_delta_matrix = active_delta_matrix.T
    active_delta_matrix = active_delta_matrix/k
    return y_hat_p, active_delta_matrix


# %%
# Training data generation
SC = SpreadingCodeGen(N, m, Nd, dv)
Codebook = NonDiagCodebook(N, m, Nd, dv, SC)
n = 20        # workers
nn = 45       # result id (num of ObjectRef)
p = 2500


@ray.remote
def Mega_Gen(start, end):
    return [TrainingGen(N, m, Nd, dv, p, k, snr) for x in range(start, end)]


start_time = time.time()
result_ids = [Mega_Gen.remote(0, n) for x in range(nn)]
y_hat_p = np.zeros((n*nn*p, 2*m))
active_delta_matrix = np.zeros((n*nn*p, N))
while len(result_ids):
    i = nn - len(result_ids)
    print('i:', i)
    done_id, result_ids = ray.wait(result_ids)
    temp = ray.get(done_id[0])
    print('doneID:', done_id)
    for j in range(n):
        y_hat_p[(i*n+j)*p:(i*n+j+1)*p, :] = temp[j][0]
        active_delta_matrix[(i*n+j)*p:(i*n+j+1)*p, :] = temp[j][1]

end_time = time.time()
print('wait', end_time - start_time, 'seconds')


# %%
# Training
early_stopping = EarlyStopping(monitor='loss', patience=25)
saveWeight = ModelCheckpoint(filepath='./' + 'AUD_' + str(k) + 'user_' + str(dv) + 'dv_' + str(snr) + 'snr_' + '.h5',
                             monitor='loss',
                             # verbose=1,
                             save_best_only=True,
                             save_weights_only=True,
                             mode='auto', save_freq=150)
reduce_lr = ReduceLROnPlateau(monitor='loss',
                              factor=0.5, patience=6,
                              min_lr=0.5*10**-7)

adam = Adam(learning_rate=5*10**-4)
AUD1 = AUD(alpha, N, m)
AUD1.summary()
# model = tf.keras.utils.plot_model(AUD1)
AUD1.compile(optimizer=adam,
             loss='categorical_crossentropy',
             metrics=['accuracy'])


start_time1 = time.time()
AUD1.fit(y_hat_p[:2400000, :], active_delta_matrix[:2400000, :],
         epochs=200,
         batch_size=2048,
         validation_split=0.25,
         callbacks=[early_stopping, reduce_lr, saveWeight])
end_time1 = time.time()

print('\nGeneration Time:', end_time - start_time, 'seconds')
print('Training Time:', end_time1 - start_time1, 'seconds\n')

hist_dict = AUD1.history
all_val_loss = hist_dict.history['val_loss']
all_loss = hist_dict.history['loss']
all_val_acc = hist_dict.history['val_accuracy']
all_acc = hist_dict.history['accuracy']

epoch = np.arange(1, len(all_loss) + 1)
plt.semilogy(epoch, all_val_loss, label='val_loss')
plt.semilogy(epoch, all_loss, label='loss')
plt.legend(loc=0)
plt.grid('true')
plt.xlabel('epochs')
plt.ylabel('Binary cross-entropy loss')
plt.show()

plt.plot(epoch, all_val_acc, label='val_accuracy')
plt.plot(epoch, all_acc, label='accuracy')
plt.legend(loc=0)
plt.grid('true')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()


# %%
# Predict Psucc
test = 2500
test_SNR = np.arange(0, 21, 2)
p_succ = np.zeros(test_SNR.shape,)
p_hat_list = []
for i in range(len(test_SNR)):
    print(test_SNR[i])
    test_input, p_real = TrainingGen(N, m, Nd, dv, test, k, test_SNR[i])
    p_hat = AUD1.predict(test_input)
    p_hat = np.argsort(-p_hat)[:, :k]
    p_hat_list.append(p_hat.tolist())
    p_temp = np.zeros([test, N], dtype='int')
    for j in range(k):
        p_temp[np.arange(test), p_hat[:, j]] = 1
    z = np.where((p_temp.reshape(test*N,)-p_real.reshape(test*N,)) == 0)
    p_succ[i] = np.mean((np.where(p_real != 0)[1] ==
                        np.where(p_temp != 0)[1]).astype(int))
    # print(np.where(
    #     np.where(p_real != 0)[1] != np.where(p_temp != 0)[1])[0].shape)


plt.title('Psucc (dv=' + str(dv) + ', ' + ' k=' + str(k) + ')')
plt.grid('true')
plt.xlabel('SNR')
plt.ylabel('Psucc')
plt.xlim(0, 20)
plt.ylim(0, 1)
plt.xticks(test_SNR, test_SNR)
plt.plot(test_SNR, p_succ, marker='o')
plt.show()
