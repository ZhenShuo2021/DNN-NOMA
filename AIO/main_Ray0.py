# -*- coding: utf-8 -*-
"""
Created on Wed Nov 17 21:35:35 2021

@author: Leo
"""

import time
import numpy as np
import ray
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, History, ModelCheckpoint, ReduceLROnPlateau
tf.compat.v1.disable_eager_execution()


m = 70
N = 100
Nd = 7
p = 1024
k = 1
dv = 3
alpha = 10*N
snr = 7  # training snr
bits = np.random.randint(2, size=[N, Nd])
ray.init(num_cpus=10)


def SpreadingCodeGen(N, m, Nd, dv):
    index = np.zeros((dv, N), dtype=int)
    index_list = []
    for i in range(N):
        index[:, i] = np.random.choice(m, dv, replace=False)
        index[:, i] = np.sort(index[:, i])

        index_list.append(index[:, i].tolist())
        for j in range(i):
            while index_list[j] == index_list[i]:
                index_list[j] = np.sort(np.random.choice(
                    m, dv, replace=False)).tolist()
    return index_list


def CodebookGen(N, m, Nd, dv, SC):
    Codebook = np.zeros([m, m*N*Nd], dtype=int)
    for i in range(N):
        for j in range(dv):
            for k in range(Nd):
                Codebook[SC[i][j], (SC[i][j] + k*m + m*Nd*i)] = 1
    return Codebook


@ray.remote
def MegaWork_Codebook(start, end):
    return [CodebookGen(N, m, Nd, dv, SC) for x in range(start, end)]


def CheckAndReturn(Codebook):
    # Check if codebook (generated by ray) is correct
    for i in range(10):
        z = np.where(Codebook[0][0] != Codebook[1][i])
        print(z[0])
        assert z[0].size == 0, 'Parallel error!'
    return Codebook[0][0]


# Training data in ray
# @ray.remote
def TrainingDataGen(N, m, Nd, dv, p, k, snr, Codebook):
    # row: user, col: Nd symbols with m channel gains
    active_index_matrix = np.zeros((k, p), dtype='int32')   # List
    # np array generated by active_index_matrix
    active_delta_matrix = np.zeros((N, p), dtype='int32')
    y_hat_p = np.zeros((2*m, p))
    N0 = 10**(-snr/10)
    for i in range(p):
        active_index = np.random.choice(N, k, replace=False)
        active_index_matrix[:, i] = active_index
        active_delta_matrix[:, i][active_index] = 1

    x = np.zeros((m*Nd*N, p), dtype='complex128')
    for i in range(p):
        x_temp = np.zeros((N, m*Nd), dtype='complex128')
        for j in (active_index_matrix[:, i]):
            bits = np.random.randint(0, 2, size=[1, Nd])*2-1
            channel = np.random.randn(1, m) + 1j*np.random.randn(1, m)
            x_temp[j, :] = np.kron(bits, channel)
        x[:, i] = x_temp.reshape(m*Nd*N,)
    y_tilde = np.dot(Codebook, x)
    noise = np.sqrt(N0/2) * (np.random.randn(*y_tilde.shape,) +
                             1j*np.random.randn(*y_tilde.shape,))
    # noise = np.sqrt(N0/2) * np.zeros(y_tilde.shape,)    # use for check (no noise)
    y_tilde = y_tilde + noise

    y_hat_p[:m, :] = np.real(y_tilde)
    y_hat_p[m:, :] = np.imag(y_tilde)

    return y_hat_p, active_delta_matrix

# choose @ray.remote to determine MegaWork or not


@ray.remote
def MegaWork_TrainingData(start, end):
    return [TrainingDataGen(N, m, Nd, dv, p, k, snr, Codebook) for x in range(start, end)]


# Generate spreading code for each user
SC = SpreadingCodeGen(N, m, Nd, dv)
Codebook = CodebookGen(N, m, Nd, dv, SC)

# # Check if codebook is correct
# for i in range(5):
#     test = np.random.randint(0, 100, size=1)
#     test = test[0]  # to int
#     if (Codebook[SC[test][0], SC[test][0]+m*Nd*test] == 1) and (
#             Codebook[SC[test][1], SC[test][1]+m*Nd*test] == 1):
#         ans = "Correct"
#     else:
#         ans = "Failure"
#     print("Codebook test: " + ans)


# Generate training data by ray
# def RayOutput():
#     # par = 4*10  # 40000 data
#     start = time.time()
#     results = [TrainingDataGen.remote(
#         N, m, Nd, dv, p, k, snr, Codebook) for x in range(1)]
#     results = ray.get(results)
#     y_hat_p = np.zeros((2*m, p))
#     active_delta_matrix = np.zeros((N, p))
#     for i in range(1):
#         for j in range(2):
#             y_hat_p[:, i*p:(i+1)*p] = results[0][0]
#             active_delta_matrix[:, i*p:(i+1)*p] = results[0][1]
#     print("duration =", time.time() - start)
#     return (y_hat_p, active_delta_matrix)

# for i in range(3):
#     (TD) = RayOutput()
#     print(i)


p = 10000
# Generate training data by ray with megawork
start = time.time()
# y_hat_p, active_delta_matrix
a = ([MegaWork_TrainingData.remote(0, 5) for i in range(2)])
a = ray.get(a)
print(time.time() - start)
y_hat_p = ray.get(y_hat_p)
active_delta_matrix = ray.get(active_delta_matrix)


# Generate training data without ray
def TrainingDataGen(N, m, Nd, dv, p, k, snr, Codebook):
    while True:
        # row: user, col: Nd symbols with m channel gains
        active_index_matrix = np.zeros((k, p), dtype='int32')   # List
        # np array generated by active_index_matrix
        active_delta_matrix = np.zeros((N, p), dtype='int32')
        y_hat_p = np.zeros((2*m, p))
        N0 = 10**(-snr/10)
        # for epoch in range(p):
        for i in range(p):
            active_index = np.random.choice(N, k, replace=False)
            active_index_matrix[:, i] = active_index
            active_delta_matrix[:, i][active_index] = 1

        x = np.zeros((m*Nd*N, p), dtype='complex128')
        for i in range(p):
            # print(i)
            x_temp = np.zeros((N, m*Nd), dtype='complex128')
            for j in (active_index_matrix[:, i]):
                bits = np.random.randint(0, 2, size=[1, Nd])*2-1
                channel = np.random.randn(1, m) + 1j*np.random.randn(1, m)
                x_temp[j, :] = np.kron(bits, channel)
            x[:, i] = x_temp.reshape(m*Nd*N,)
        y_tilde = np.dot(Codebook, x)
        noise = np.sqrt(N0/2) * (np.random.randn(*y_tilde.shape,) +
                                 1j*np.random.randn(*y_tilde.shape,))
        # noise = np.sqrt(N0/2) * np.zeros(y_tilde.shape,)    # use for check (no noise)
        y_tilde = y_tilde + noise

        y_hat_p[:m, :] = np.real(y_tilde)
        y_hat_p[m:, :] = np.imag(y_tilde)
        y_hat_p = y_hat_p.T
        active_delta_matrix = active_delta_matrix.T
        yield (y_hat_p, active_delta_matrix)
    # return y_hat_p, active_delta_matrix


y_hat_p, active_delta_matrix = TrainingDataGen(
    N, m, Nd, dv, p, k, snr, Codebook)


# Time test
# start = time.time()
# y_hat_p, active_delta_matrix = TrainingDataGen(N, m, Nd, dv, 2*10**4, k, snr, Codebook)
# print("duration =", time.time() - start)

# y_hat_p = y_hat_p.T
# active_delta_matrix = active_delta_matrix.T


def Hidden_Layer(input_tensor, alpha, stage):
    """
    Parameters
    ----------
    input_tensor : Output of last layer
    alpha : Number of neuron
    stage : Index of hidden layer

    Returns output tensor
    -------
    """
    name_base = 'HL' + stage
    x = layers.Dense(alpha, name=name_base + '_1')(input_tensor)
    x = layers.BatchNormalization(name=name_base + '_2')(x)
    x = layers.Activation('relu', name=name_base + '_3')(x)
    x = layers.Dropout(0.2, name=name_base + '_4')(x)
    x = layers.add([x, input_tensor], name=name_base + '_Add')
    return x


def AUD(alpha, N):
    model_input = layers.Input(shape=[2*m, ], name='InputLayer')
    x = layers.Dense(alpha, name='InputFC')(model_input)
    x = layers.BatchNormalization(name='InputBN')(x)
    x = Hidden_Layer(x, alpha, stage='_A')
    x = Hidden_Layer(x, alpha, stage='_B')
    x = Hidden_Layer(x, alpha, stage='_C')
    x = Hidden_Layer(x, alpha, stage='_D')
    x = Hidden_Layer(x, alpha, stage='_E')
    x = Hidden_Layer(x, alpha, stage='_F')
    x = layers.Dense(N, name='OutputFC')(x)
    x = layers.Softmax(axis=-1, name='OutputActivation')(x)

    model = Model(model_input, x, name='D_AUD')
    return model


early_stopping = EarlyStopping(monitor='val_loss', patience=10)
reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5,
                              patience=7, min_lr=0.000001)

ray.shutdown()

AUD1 = AUD(alpha, N)
AUD1.compile(optimizer=Adam(learning_rate=5*10**-4),
             loss='categorical_crossentropy')

# AUD1.fit(y_hat_p,active_delta_matrix, # 一次產生p筆資料
#          # steps_per_epoch=9765, # 一個epoch會跑training_gen七次
#          epochs=10,
#          batch_size=1024,
#          validation_split=0.3,
#          # validation_data=TrainingDataGen(N, m, Nd, dv, p, k, snr, Codebook),
#          # validation_steps=2929,
#          callbacks=[early_stopping, reduce_lr])
AUD1.fit(TrainingDataGen(N, m, Nd, dv, p, k, snr, Codebook),  # 一次產生p筆資料
         steps_per_epoch=1,  # 一個epoch會跑training_gen七次
         epochs=100,
         # batch_size=1024,
         # validation_split=0.3,
         validation_data=TrainingDataGen(N, m, Nd, dv, p, k, snr, Codebook),
         validation_steps=10,
         callbacks=[early_stopping, reduce_lr])
AUD1.save_weights('./first_weight.h5')
hist_dict = AUD1.history
all_val_loss = hist_dict.history['val_loss']
all_loss = hist_dict.history['loss']
